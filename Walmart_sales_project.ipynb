{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb381d34",
   "metadata": {},
   "source": [
    "# Projet Walmart sales\n",
    "\n",
    "Bloc3 : PROJECTS Supervised Machine Learning\n",
    "\n",
    "Walmart sales\n",
    "\n",
    "360 min\n",
    "\n",
    "https://app.jedha.co/course/projects-supervised-machine-learning-ft/walmart-sales-ft\n",
    "\n",
    "## Company's Description ðŸ“‡\n",
    "\n",
    "Walmart Inc. is an American multinational retail corporation that operates a chain of hypermarkets, discount department stores, and grocery stores from the United States, headquartered in Bentonville, Arkansas. The company was founded by Sam Walton in 1962.\n",
    "\n",
    "## Project ðŸš§\n",
    "\n",
    "Walmart's marketing service has asked you to build a machine learning model able to estimate the weekly sales in their stores, with the best precision possible on the predictions made. Such a model would help them understand better how the sales are influenced by economic indicators, and might be used to plan future marketing campaigns.\n",
    "\n",
    "## Goals ðŸŽ¯\n",
    "\n",
    "The project can be divided into three steps:\n",
    "\n",
    "- Part 1 : make an EDA and all the necessary preprocessings to prepare data for machine learning\n",
    "- Part 2 : train a **linear regression model** (baseline)\n",
    "- Part 3 : avoid overfitting by training a **regularized regression model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df7ada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries for EDA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "#from src.eda import *\n",
    "# import libraries for modeling\n",
    "# preprocessing selection\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# model selection\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "# model evaluation\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98728ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04476153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def how_null_is_it(df: pd.DataFrame):\n",
    "        print()\n",
    "        print(f\"Overall missing values in dataset : {df.isnull().sum().sum()}\")\n",
    "        print( )\n",
    "        print(\"missing values in dataset per column :\")\n",
    "        print(df.isnull().sum() )\n",
    "        print( )\n",
    "\n",
    "def summary(df: pd.DataFrame):\n",
    "        \"\"\"Print a summary of the dataset.\"\"\"\n",
    "        print(\"________________________________________________\" )\n",
    "        print(\"Data Start\")\n",
    "        display(df.head(10) )\n",
    "        print()\n",
    "        print(\"Data End\")\n",
    "        display(df.tail(10) )\n",
    "        print()\n",
    "        print(\"shape of the dataset : \")\n",
    "        display(df.shape)\n",
    "        print()\n",
    "        print(\"columns of the dataset : \")\n",
    "        display(df.columns)\n",
    "        print()\n",
    "        print(\"data describe : \")\n",
    "        display(df.describe(include='all') )\n",
    "        print()\n",
    "        print(\"types in dataset :\")\n",
    "        display(df.dtypes)\n",
    "        print()\n",
    "        print(f\"Overall missing values in dataset : {df.isnull().sum().sum()}\")\n",
    "        print( )\n",
    "        print(\"missing values in dataset per column :\")\n",
    "        display(df.isnull().sum() )\n",
    "        print(\"________________________________________________\" )\n",
    "def score_model(model,x_train, y_train, x_test, y_test):\n",
    "    print(model.score(x_train, y_train))\n",
    "    print(model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d9bf49",
   "metadata": {},
   "source": [
    "## Part 0 : import dataset and inception\n",
    "\n",
    "We import the csv dataset into a pandas dataframe and have a first look to the datastructure.\n",
    "Since we know that original data come from Kaggle before modifying by Jedha, we watch in Kaggle about the meaning of eaxh column. \n",
    "This is the historical data that covers sales from 2010-02-05 to 2012-11-01, in the file Walmart_Store_sales. Within this file you will find the following fields:\n",
    "\n",
    "\n",
    " |  column name   |   description   |  \n",
    " |  -------- | ------- |\n",
    " | Store | the store number | \n",
    " | Date | the week of sales | \n",
    " | Weekly_Sales  |  sales for the given store | \n",
    " |  Holiday_Flag  | whether the week is a special holiday week <br /> 1 â€“ Holiday week <br /> 0 â€“ Non-holiday week | \n",
    " | Temperature |  Temperature on the day of sale | \n",
    " | Fuel_Price  |  Cost of fuel in the region | \n",
    " | CPI |  Prevailing consumer price index  | \n",
    " | Unemployment  |  Prevailing unemployment rate  | \n",
    " | Holiday Events  |    Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13<br /> Labour Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13<br /> Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13<br /> Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13  | \n",
    "\n",
    " L'indice des prix Ã  la consommation ou IPC (en anglais, consumer price index ou CPI) mesure l'Ã©volution du niveau moyen des prix des biens et services consommÃ©s par les mÃ©nages, pondÃ©rÃ©s par leur part dans la consommation moyenne des mÃ©nages. L'indice (105 par exemple) permet de mesurer l'inflation (ici +5 % de hausse des prix), ou la dÃ©flation en cas de baisse des prix, sur une pÃ©riode et donc l'Ã©volution de la valeur de la monnaie (la valeur de la monnaie diminue lorsque les prix augmentent). Le taux (annuel) d'inflation dÃ©signe gÃ©nÃ©ralement, lorsque l'indice n'est pas prÃ©cisÃ©, le pourcentage d'augmentation de cet indice (IPC) particulier sur une annÃ©e.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e95b108",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4170e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"Data/Walmart_Store_sales.csv\")\n",
    "summary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcf7cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73d1fbf",
   "metadata": {},
   "source": [
    "We noticed also that the Store is an integer id to identify the store. We convert it into intger since the store sounds more like a classification than a regression value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e8e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()\n",
    "df.reset_index(drop=True)\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d81bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({\"Store\": int, \"Weekly_Sales\": float, \"Temperature\": float, \"Fuel_Price\": float, \"CPI\": float, \"Unemployment\": float},errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea691b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2100d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Holiday_Flag'] = df['Holiday_Flag'].astype('int64')\n",
    "df.dtypes\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a574845",
   "metadata": {},
   "source": [
    "The date is an object time, we need to convert in real date figures (new column dt) from the string format _day_-_month_-_year_ (`\"%d-%m-%Y\"`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ff6ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb08ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dt'] = pd.to_datetime(df[\"Date\"], format = \"%d-%m-%Y\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58be464b",
   "metadata": {},
   "source": [
    "We split the date on year, month and week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b0ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'] = df['dt'].dt.year.astype('Int64')\n",
    "df['Month'] = df['dt'].dt.month.astype('Int64')\n",
    "df['Day'] = df['dt'].dt.day.astype('Int64')\n",
    "df['Week'] = df['dt'].dt.isocalendar().week.astype('Int64')\n",
    "df['DayOfWeek'] = df['dt'].dt.weekday.astype('Int64')\n",
    "\n",
    "df.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a5178",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)\n",
    "print()\n",
    "display(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a11dc5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74fa262e",
   "metadata": {},
   "source": [
    "## Part 1 : Exploration, Exploratory data analysis (EDA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6af813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant for the layout of the plots\n",
    "WIDTH = 600\n",
    "HEIGHT = 400\n",
    "MARGIN = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb7bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute total revenue cumulated by day\n",
    "df_store = df.groupby('Store')['Week'].count().reset_index()\n",
    "df_store.sort_values(by='Store',ascending= False)\n",
    "fig = px.pie(df_store, names = \"Store\", values = \"Week\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_store_by_weekly_sales=df.groupby(['Store'])[['Weekly_Sales']].mean()\n",
    "orders_store_by_weekly_sales.sort_values(by='Weekly_Sales',ascending= False)\n",
    "orders_year_by_weekly_sales=df.groupby(['Year'])[['Weekly_Sales']].mean()\n",
    "orders_month_by_weekly_sales=df.groupby(['Month'])[['Weekly_Sales']].mean()\n",
    "orders_week_by_weekly_sales=df.groupby(['Week'])[['Weekly_Sales']].mean()\n",
    "orders_week_by_weekly_sales.sort_values(by='Weekly_Sales',ascending= False)\n",
    "\n",
    "#px.bar(orders_year_by_weekly_sales, x= orders_year_by_weekly_sales.index, y='Weekly_Sales', \n",
    "#        title=\"Average Weekly Sales by Year\", labels={\"x\":\"Store\",\"Weekly_Sales\":\"Average Weekly Sales per Year\"}).show()\n",
    "#px.bar(orders_store_by_weekly_sales, x= orders_store_by_weekly_sales.index, y='Weekly_Sales', title=\"Average Weekly Sales by Store\", labels={\"x\":\"Store\",\"Weekly_Sales\":\"Average Weekly Sales per store\"}).show()\n",
    "#px.bar(orders_month_by_weekly_sales, x= orders_month_by_weekly_sales.index, y='Weekly_Sales', title=\"Average Weekly Sales by Month\", labels={\"x\":\"Month\",\"Weekly_Sales\":\"Average Weekly Sales per month\"}).show()\n",
    "#px.bar(orders_week_by_weekly_sales, x= orders_week_by_weekly_sales.index, y='Weekly_Sales', title=\"Average Weekly Sales\", labels={\"x\":\"Month\",\"Weekly_Sales\":\"Average Weekly Sales\"}).show()\n",
    "\n",
    "fig = make_subplots(rows = 4, cols = 1, subplot_titles = ([\"Wallmart average weekly sales per store\",\n",
    "                                        \"Wallmart average weekly sales per year\",\n",
    "                                        \"Wallmart average weekly sales per month\",\n",
    "                                        \"Wallmart average weekly sales per week\"] ))\n",
    "fig.add_bar(\n",
    "\n",
    "        x = orders_store_by_weekly_sales.index,\n",
    "        y = orders_store_by_weekly_sales.Weekly_Sales,\n",
    "        row = 1,\n",
    "        col = 1\n",
    "\n",
    ")    \n",
    "fig.add_bar(\n",
    "        x = orders_year_by_weekly_sales.index,\n",
    "        y = orders_year_by_weekly_sales['Weekly_Sales'],\n",
    "        row = 2,\n",
    "        col = 1\n",
    "\n",
    ")    \n",
    "fig.add_bar(\n",
    "\n",
    "        x = orders_month_by_weekly_sales.index,\n",
    "        y = orders_month_by_weekly_sales['Weekly_Sales'],\n",
    "        row = 3,\n",
    "        col = 1\n",
    ")    \n",
    "fig.add_bar(\n",
    "        x = orders_week_by_weekly_sales.index,\n",
    "        y = orders_week_by_weekly_sales['Weekly_Sales'],\n",
    "        row = 4,\n",
    "        col = 1\n",
    ")    \n",
    "layout = go.Layout(\n",
    "    title = go.layout.Title(text = \"Average Weekly Sales\", x = 1.0),\n",
    "    showlegend = False,\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=2000,\n",
    "    xaxis=go.layout.XAxis(linecolor=\"black\", linewidth=1, mirror=True),\n",
    "    yaxis=go.layout.YAxis(linecolor=\"black\", linewidth=1, mirror=True),\n",
    "    margin=go.layout.Margin(l=50, r=50, b=100, t=100, pad=4),\n",
    ")\n",
    "\n",
    "fig.update_layout(layout)\n",
    "#px.bar(orders_store_by_weekly_sales, x= orders_store_by_weekly_sales.index, y='Weekly_Sales', title=\"Average Weekly Sales by Store\", labels={\"x\":\"Store\",\"Weekly_Sales\":\"Average Weekly Sales\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351d7b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_month_by_weekly_sales=df.groupby(['Month'])[['Weekly_Sales']].sum()\n",
    "orders_month_by_weekly_sales.sort_values(by='Weekly_Sales',ascending= False)\n",
    "fig = px.line(orders_month_by_weekly_sales, x=orders_month_by_weekly_sales.index, y='Weekly_Sales', title=\"Total Weekly Sales by Month\", width=WIDTH, height=HEIGHT)\n",
    "fig = px.scatter(orders_month_by_weekly_sales, x=orders_month_by_weekly_sales.index, y='Weekly_Sales', title=\"Total Weekly Sales by Month\", width=WIDTH, height=HEIGHT)\n",
    "#fig.update_traces(marker=dict(size=12, color='LightSkyBlue', line=dict(width=2, color='DarkSlateGrey')), selector=dict(mode='markers'))\n",
    "fig.update_layout(margin=dict(l=MARGIN, r=MARGIN, t=MARGIN, b=MARGIN))\n",
    "px.density_heatmap(orders_month_by_weekly_sales, x=orders_month_by_weekly_sales.index, y='Weekly_Sales', nbinsx=20, nbinsy=10, width=WIDTH, height=HEIGHT)\n",
    "px.box(df, x='Month', y='Weekly_Sales', width=WIDTH, height=HEIGHT)\n",
    "fig = px.line(orders_month_by_weekly_sales, x=orders_month_by_weekly_sales.index, y='Weekly_Sales', title=\"Total Weekly Sales by Month\", width=WIDTH, height=HEIGHT)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f68aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_month_by_weekly_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a0aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_week_by_weekly_sales=df.groupby(['Week'])[['Weekly_Sales']].sum()\n",
    "orders_week_by_weekly_sales.sort_values(by='Weekly_Sales',ascending= False)\n",
    "fig = px.line(orders_week_by_weekly_sales, x=orders_week_by_weekly_sales.index, y='Weekly_Sales', title=\"Total Weekly Sales by week\", width=WIDTH, height=HEIGHT)\n",
    "fig = px.scatter(orders_week_by_weekly_sales, x=orders_week_by_weekly_sales.index, y='Weekly_Sales', title=\"Total Weekly Sales by week\", width=WIDTH, height=HEIGHT)\n",
    "#fig.update_traces(marker=dict(size=12, color='LightSkyBlue', line=dict(width=2, color='DarkSlateGrey')), selector=dict(mode='markers'))\n",
    "fig.update_layout(margin=dict(l=MARGIN, r=MARGIN, t=MARGIN, b=MARGIN))\n",
    "px.density_heatmap(orders_week_by_weekly_sales, x=orders_week_by_weekly_sales.index, y='Weekly_Sales', nbinsx=20, nbinsy=10, width=WIDTH, height=HEIGHT)\n",
    "px.box(df, x='Week', y='Weekly_Sales', width=WIDTH, height=HEIGHT)\n",
    "fig = px.line(orders_week_by_weekly_sales, x=orders_week_by_weekly_sales.index, y='Weekly_Sales', title=\"Total Weekly Sales by Week\", width=WIDTH, height=HEIGHT)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2144e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_week_by_weekly_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb5f63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(orders_week_by_weekly_sales, x=orders_week_by_weekly_sales.index, y='Weekly_Sales' , title='Weekly Sales according to week', width=WIDTH, height=HEIGHT)\n",
    "fig.update_layout(margin=dict(l=MARGIN, r=MARGIN, t=MARGIN, b=MARGIN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c567dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(orders_month_by_weekly_sales, x=orders_month_by_weekly_sales.index, y='Weekly_Sales' , title='Weekly Sales according to month', width=WIDTH, height=HEIGHT)\n",
    "fig.update_layout(margin=dict(l=MARGIN, r=MARGIN, t=MARGIN, b=MARGIN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c9827a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaed2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcpi=df[df['CPI'].notna()]\n",
    "cpi_over_store = dfcpi.groupby('Store')['CPI'].mean()\n",
    "display(cpi_over_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f466c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.density_heatmap(df, x='Store', y='Weekly_Sales', nbinsx=20, nbinsy=40, width=WIDTH, height=HEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864dd220",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_month_by_weekly_sales=df.groupby(['Year','Month'])['Weekly_Sales'].sum()\n",
    "display(orders_month_by_weekly_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdfd573",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_month_by_weekly_sales.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4515cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "year=2010\n",
    "orders_month_by_weekly_sales.loc[year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb15b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_month_by_weekly_sales=df.groupby(['Year','Month'])['Weekly_Sales'].sum()\n",
    "dfm=orders_month_by_weekly_sales\n",
    "years = [2010, 2011, 2012]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "visible = True, False, False, False, False\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    if i == 0:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=dfm[year].index,\n",
    "                y=dfm,\n",
    "                visible=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=dfm[year].index,\n",
    "                y=dfm,\n",
    "                visible=False\n",
    "            )\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    updatemenus=[go.layout.Updatemenu(\n",
    "        active=0,\n",
    "        buttons=[\n",
    "            go.layout.updatemenu.Button(\n",
    "                label=\"2010\",\n",
    "                method='update',\n",
    "                args=[{'visible': [True,  False, False],\n",
    "                        \"xaxis\": dict(range=[1, 12], title=\"Month\", tick0=1,dtick=1)\n",
    "                }]\n",
    "            ),\n",
    "            go.layout.updatemenu.Button(\n",
    "                label=\"2011\",\n",
    "                method='update',\n",
    "                args=[{'visible': [False,  True,  False],\n",
    "                        \"xaxis\": dict(range=[1, 12], title=\"Month\", tick0=1,dtick=1)\n",
    "                }]\n",
    "            ),\n",
    "            go.layout.updatemenu.Button(\n",
    "                label=\"2012\",\n",
    "                method='update',\n",
    "                args=[{'visible': [False, False,  True],\n",
    "                        \"xaxis\": dict(range=[1, 12], title=\"Month\", tick0=1,dtick=1)\n",
    "                }]\n",
    "            ),\n",
    "            \n",
    "        ]\n",
    "    )]\n",
    ")\n",
    "\n",
    "fig.update_layout(title=dict(text=\"Monthly Sales observations in a chosen year\", x=0.5))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a04ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_week_by_weekly_sales=df.groupby(['Year','Week'])['Weekly_Sales'].sum()\n",
    "dfw=orders_week_by_weekly_sales\n",
    "years = [2010, 2011, 2012]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "visible = True, False, False, False, False\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    if i == 0:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=dfw[year].index,\n",
    "                y=dfw,\n",
    "                visible=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=dfw[year].index,\n",
    "                y=dfw,\n",
    "                visible=False\n",
    "            )\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    updatemenus=[go.layout.Updatemenu(\n",
    "        active=0,\n",
    "        buttons=[\n",
    "            go.layout.updatemenu.Button(\n",
    "                label=\"2010\",\n",
    "                method='update',\n",
    "                args=[{'visible': [True,  False, False],\n",
    "                        \"xaxis\": dict(range=[1, 52], title=\"Week\", tick0=1,dtick=4)\n",
    "                }]\n",
    "            ),\n",
    "            go.layout.updatemenu.Button(\n",
    "                label=\"2011\",\n",
    "                method='update',\n",
    "                args=[{'visible': [False,  True,  False],\n",
    "                        \"xaxis\": dict(range=[1, 52], title=\"Week\", tick0=1,dtick=4)\n",
    "                }]\n",
    "            ),\n",
    "            go.layout.updatemenu.Button(\n",
    "                label=\"2012\",\n",
    "                method='update',\n",
    "                args=[{'visible': [False, False,  True],\n",
    "                       \"xaxis\": dict(range=[1, 52], title=\"Week\", tick0=1,dtick=4)\n",
    "                }]\n",
    "            ),\n",
    "            \n",
    "        ]\n",
    "    )]\n",
    ")\n",
    "\n",
    "fig.update_layout(title=dict(text=\"Weekly Sales observations in a chosen year\", x=0.5))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00190dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b3b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c=df.drop(columns=['dt','Date'])\n",
    "# Correlation\n",
    "df_corr = df_c.corr().round(1)  \n",
    "# Mask to matrix\n",
    "mask = np.zeros_like(df_corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "# Viz\n",
    "df_corr_viz = df_corr.mask(mask).dropna(how='all').dropna( how='all')\n",
    "fig = px.imshow(df_corr_viz, text_auto=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddce96b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3c9b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(df_c.corr())\n",
    "\n",
    "# 0.0 - 0.3 = No correlation\n",
    "# 0.3 - 0.5 = Weak correlation\n",
    "# 0.5 - 0.7 = Moderate correlation\n",
    "# 0.7 - 1.0 = Strong correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42efebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_per_week=df.groupby(['Year','Week'])[['Weekly_Sales','CPI',\"Temperature\"]].sum()\n",
    "df_spw=Sales_per_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531dc397",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_matrix(df, width=1200, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c100328",
   "metadata": {},
   "source": [
    "## Preprocessing - pandas part ðŸ¼ðŸ¼ \n",
    "In this dataset, some features are removed since they are useless for the modelling.\n",
    "\n",
    "### Remove row where target values are missing\n",
    "\n",
    "Drop lines where target values are missing :\n",
    "\n",
    "Here, the target variable (y) corresponds to the column Weekly_Sales. One can see above that there are some missing values in this column.\n",
    "We never use imputation techniques on the target : it might create some bias in the predictions !\n",
    "Then, we will just drop the lines in the dataset for which the value in Weekly_Sales is missing.\n",
    "\n",
    "We noticed that 14 Weekly_Sales are missing in the dataset.\n",
    "\n",
    "Since this is the target value we have no other choice than remove thes rows from the original dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19cdeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df[df['Weekly_Sales'].notna()]\n",
    "print(df_model['Weekly_Sales'].isnull().sum())\n",
    "df_model.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa983d53",
   "metadata": {},
   "source": [
    "### Remove duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625d8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rs,cs = df.shape\n",
    "\n",
    "df_model.drop_duplicates(inplace=True)\n",
    "\n",
    "if df_model.shape==(rs,cs):\n",
    "    print('\\n\\033[1mInference:\\033[0m The dataset doesn\\'t have any duplicates')\n",
    "else:\n",
    "    print(f'\\n\\033[1mInference:\\033[0m Number of duplicates dropped/fixed ---> {rs-df_model.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf46e8",
   "metadata": {},
   "source": [
    "### Remove row with Not Available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594d2e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model=df_model.dropna()\n",
    "df_model.reset_index(drop=True)\n",
    "df_model.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee0cb7",
   "metadata": {},
   "source": [
    "### Remove Date with date format and wrong format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552c49d4",
   "metadata": {},
   "source": [
    "Remove row with Not available Date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b2736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df.dropna(subset=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a704306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model=df_model.drop('Date', axis=1)\n",
    "df_model=df_model.drop('dt', axis=1)\n",
    "df_model.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c7b365",
   "metadata": {},
   "source": [
    "### Holidays Flag analysis\n",
    "\n",
    "There is a very small correlation between Weekly_Sales and Holiday_Flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02001970",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Moreover there is only  {df_model['Holiday_Flag'].sum()}  holidays rows  in the dataset \" )\n",
    "print(f\"and {df_model['Holiday_Flag'].isna().sum()} Non available values over {df_model.shape[0]} rows.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19da3eef",
   "metadata": {},
   "source": [
    "We can remove Holidays data from this dataset which is more a perturbation data than a descriptive data for the Weekly Sales prediction\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2152033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_model = df_model.drop(columns=['Holiday_Flag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b4b790",
   "metadata": {},
   "source": [
    "### Fuel Price analysis\n",
    "\n",
    "There is no correlation between Weekly_Sales and Fuel Price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06342024",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Moreover there is {df_model['Fuel_Price'].isna().sum()} Non available values over {df_model.shape[0]} rows.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff61f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_model.drop(columns=['Fuel_Price'])\n",
    "how_null_is_it(df_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9432b8c8",
   "metadata": {},
   "source": [
    "We can remove Fuel Price from this dataset data which is more a perturbation data than a descriptive data for the Weekly Sales prediction\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fedcd2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df_model['Weekly_Sales']\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1484c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "879374af",
   "metadata": {},
   "source": [
    "### Outlier analysis\n",
    "\n",
    "We track outliers to remove these rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9cfdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_list = ['Weekly_Sales', 'Temperature', 'CPI', 'Unemployment']\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "for col in numeric_list:\n",
    "    outliers = detect_outliers_iqr(df_model, col)\n",
    "    print(f\"{col} -> Outlier : {outliers.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c73763f",
   "metadata": {},
   "source": [
    "##### Unemployement outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc4501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outliers_unemp = detect_outliers_iqr(df_model, 'Unemployment')\n",
    "outliers_unemp.index\n",
    "#\n",
    "df_model=df_model.drop(outliers_unemp.index,axis=0)\n",
    "df_model.reset_index(drop=True)\n",
    "display(df_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5a4356",
   "metadata": {},
   "source": [
    "## Preprocessing - scikit-learn part ðŸ”¬ðŸ”¬\n",
    "We will use ColumnTransformer and Pipeline from sklearn to preprocess the data before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea3f38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x=df_model.drop('Weekly_Sales', axis=1)\n",
    "y=df_model['Weekly_Sales']\n",
    "x_train, x_test, y_train, y_test= train_test_split(x,y, test_size=0.2, random_state= 42)\n",
    "print(\"Train set:\", x_train.shape, y_train.shape)\n",
    "print(\"Test set:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074497cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbfeddf",
   "metadata": {},
   "source": [
    "Create the preprocessing pipeline for numeric columns\n",
    "\n",
    "* list of numerical columns\n",
    "* impute numeric -> median\n",
    "* standardise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1965fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = x[['Store','Temperature', 'CPI', 'Unemployment', 'Year', 'Month', 'Week','Day','DayOfWeek']].columns.tolist() #x.select_dtypes(include=np.number).columns.tolist()\n",
    "#numerical_columns =x.select_dtypes(exclude=\"object\").columns\n",
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb7e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "numerical_scaler = StandardScaler()\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('num_imputer', numeric_imputer),\n",
    "    ('num_scaler', numerical_scaler)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea42dac",
   "metadata": {},
   "source": [
    "Create the preprocessing pipeline for category columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873112d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical_columns = x.select_dtypes(include=\"object\").columns #x.select_dtypes(exclude=np.number).columns.tolist()\n",
    "categorical_columns = x[['Store','Holiday_Flag']].columns.tolist()\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36378e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "#categorical_imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "\n",
    "categorical_encoder = OneHotEncoder(drop='first')\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('cat_imputer', categorical_imputer),\n",
    "    ('cat_encoder', categorical_encoder)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009576d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ColumnTransformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_pipeline, numerical_columns),\n",
    "        (\"cat\", categorical_pipeline, categorical_columns),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7742e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test)\n",
    "print()\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc02567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessings on train set\n",
    "print(\"Performing preprocessings on train set...\")\n",
    "print(x_train.head())\n",
    "x_train = preprocessor.fit_transform(x_train)\n",
    "print(\"...Done.\")\n",
    "print(\n",
    "    x_train[0:5]\n",
    ")  # MUST use this syntax because X_train is a numpy array and not a pandas DataFrame anymore\n",
    "print()\n",
    "\n",
    "# Preprocessings on test set\n",
    "print(\"Performing preprocessings on test set...\")\n",
    "print(x_test.head())\n",
    "x_test = preprocessor.transform(x_test)  # Don't fit again !! The test set is used for validating decisions\n",
    "# we made based on the training set, therefore we can only apply transformations that were parametered using the training set.\n",
    "# Otherwise this creates what is called a leak from the test set which will introduce a bias in all your results.\n",
    "print(\"...Done.\")\n",
    "print(\n",
    "    x_test[0:5, :]\n",
    ")  # MUST use this syntax because X_test is a numpy array and not a pandas DataFrame anymore\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d5ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78bb5e",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "We start training a baseline model, we'll analyze the results and we'll build improved model.\n",
    "###  Baseline model (linear regression)\n",
    "Once you've trained a first model, don't forget to assess its performances on the train and test sets. Are you satisfied with the results ?\n",
    "Besides, it would be interesting to analyze the values of the model's coefficients to know what features are important for the prediction. To do so, the `.coef_` attribute of scikit-learn's LinearRegression class might be useful. Please refer to the following link for more information ðŸ˜‰ https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd69d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46330f2e",
   "metadata": {},
   "source": [
    "#### Model estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d188df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "score_model(model,x_train, y_train, x_test, y_test)\n",
    "print(f\"MAE  : {mae:.2f}\")\n",
    "print(f\"MSE  : {mse:.2f}\")\n",
    "print(f\"RMSE : {rmse:.2f}\")\n",
    "print(f\"RÂ²   : {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b977fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.columns)\n",
    "print()\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b6611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc7f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({\n",
    "    'Feature': preprocessor.get_feature_names_out(),\n",
    "    'Coefficient': model.coef_\n",
    "}).sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Linear Regression Coefficients):\")\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73669ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342c10ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(x=y_test, y=y_pred, title=\"Actual vs Predicted Sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9e3d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Ridge(alpha=0.005)\n",
    "regressor.fit(x_train, y_train)\n",
    "scores = cross_val_score(regressor, x_train, y_train, cv=3)\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Average cross-validation score:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b312e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Ridge()\n",
    "\n",
    "params = {\n",
    "    \"alpha\": [0,0.05,0.2, 0.5, 1, 1.5, 2, 3]\n",
    "}\n",
    "\n",
    "gridsearch = GridSearchCV(regressor, param_grid = params, cv = 3) # cv : the number of folds to be used for CV\n",
    "gridsearch.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ef13bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters : \", gridsearch.best_params_)\n",
    "print(\"Best R2 score : \", gridsearch.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f933bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(gridsearch.cv_results_).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e039298",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Lasso()\n",
    "\n",
    "params = {\n",
    "    'alpha': [0.05,0.1,0.3,0.5,0.8, 1.4, 1.5, 1.7] \n",
    "}\n",
    "\n",
    "gridsearch = GridSearchCV(regressor, param_grid = params, cv = 3) # cv : the number of folds to be used for CV\n",
    "gridsearch.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a04010",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(gridsearch.cv_results_).T.iloc[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5478c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_regressor = gridsearch.best_estimator_\n",
    "y_test_pred = best_regressor.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d11f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0457efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feature_importance = pd.DataFrame(\n",
    "    {\n",
    "        'feature': preprocessor.get_feature_names_out(),\n",
    "        'coef_linear': model.coef_,\n",
    "        'coef_lasso': best_regressor.coef_\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c85edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_keep = best_feature_importance[best_feature_importance['coef_lasso'] > 0]['feature'].tolist()\n",
    "features_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1497d563",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = x[features_to_keep]\n",
    "\n",
    "xr_train, xr_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "xr_train = preprocessor.fit_transform(xr_train)\n",
    "xr_test = preprocessor.transform(xr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c9535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_regressor = LinearRegression()\n",
    "\n",
    "final_regressor.fit(xr_train, y_train)\n",
    "\n",
    "print(final_regressor.score(xr_train, y_train))\n",
    "print(final_regressor.score(xr_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a08780",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lasso = Lasso(alpha=1)\n",
    "\n",
    "final_lasso.fit(xr_train, y_train)\n",
    "\n",
    "print(final_lasso.score(xr_train, y_train))\n",
    "print(final_lasso.score(xr_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jedha-dsfs-ft-35",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
